{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88a072da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn             #muntar xarxes (capes, activacions, backpropagació de gradients...)\n",
    "import torch.optim as optim       #escollir optimitzador que recalcularà els pesos\n",
    "import torch.nn.functional as F   #cridar directament a funcions sense acudir a les classes\n",
    "#import torch.utils.data as data   #muntar dataloaders que generaran els batches de dades\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision  #eines diverses per descarregar bases de dades, transformar dades...\n",
    "\n",
    "import matplotlib.pyplot as plt   #mostrar, plotejar, displayar dades i imatges\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "816d1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =    5                 #per simplificar farem servir mateixa mida per training i test \n",
    "learning_rate = 0.01                #tasa d'aprenentatge\n",
    "momentum =      0.1                  #paràmetre pel cas de l'optimitzador SGD (Stochastic gradient descent)\n",
    "n_epochs =      10                  #vegades que la xarxa veurà totes les dades d'entrenament\n",
    "criterium = nn.CrossEntropyLoss() #loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbc3f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.imgs_path = path\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        print(file_list)\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path[len(path):]\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        #print(self.data)\n",
    "        for d in self.data:\n",
    "            print(d)\n",
    "        self.class_map = {\"negativo\" : 0, \"positivo\": 1, \"tijeras\": 2}\n",
    "        self.img_dim = (480, 480)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, self.img_dim)\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        img_tensor = img_tensor.permute(2, 0, 1)\n",
    "        class_id = torch.tensor([class_id])\n",
    "        return img_tensor, class_id\n",
    "        #return img_tensor.float(), class_id.float()     # quizas necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d37f2651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frames\\\\negativo', 'frames\\\\positivo', 'frames\\\\tijeras']\n",
      "['frames\\\\negativo\\\\IMG_0000.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0005.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0010.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0015.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0020.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0025.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0030.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0035.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0040.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0045.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0050.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0055.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0060.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0065.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0070.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0075.jpg', 'negativo']\n",
      "['frames\\\\negativo\\\\IMG_0080.jpg', 'negativo']\n",
      "['frames\\\\positivo\\\\IMG_0000.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0005.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0010.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0015.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0020.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0025.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0030.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0035.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0040.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0045.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0050.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0055.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0060.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0065.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0070.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0075.jpg', 'positivo']\n",
      "['frames\\\\positivo\\\\IMG_0080.jpg', 'positivo']\n",
      "['frames\\\\tijeras\\\\IMG_0000.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0005.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0010.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0015.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0020.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0025.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0030.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0035.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0040.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0045.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0050.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0055.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0060.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0065.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0070.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0075.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0080.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0085.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0090.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0095.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0100.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0105.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0110.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0115.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0120.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0125.jpg', 'tijeras']\n",
      "['frames\\\\tijeras\\\\IMG_0130.jpg', 'tijeras']\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(\"frames/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d68c780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[255, 255, 255,  ..., 253, 253, 253],\n",
       "          [255, 255, 255,  ..., 253, 253, 253],\n",
       "          [255, 255, 255,  ..., 253, 253, 253],\n",
       "          ...,\n",
       "          [254, 254, 254,  ..., 129, 139, 149],\n",
       "          [254, 254, 254,  ..., 131, 141, 151],\n",
       "          [254, 254, 254,  ..., 132, 144, 153]],\n",
       " \n",
       "         [[255, 255, 255,  ..., 253, 253, 253],\n",
       "          [255, 255, 255,  ..., 253, 253, 253],\n",
       "          [255, 255, 255,  ..., 253, 253, 253],\n",
       "          ...,\n",
       "          [255, 255, 255,  ..., 123, 133, 143],\n",
       "          [255, 255, 255,  ..., 125, 135, 145],\n",
       "          [255, 255, 255,  ..., 125, 135, 144]],\n",
       " \n",
       "         [[255, 255, 255,  ..., 253, 253, 253],\n",
       "          [255, 255, 255,  ..., 253, 253, 253],\n",
       "          [255, 255, 255,  ..., 253, 253, 253],\n",
       "          ...,\n",
       "          [253, 253, 253,  ..., 172, 182, 192],\n",
       "          [253, 253, 253,  ..., 176, 184, 194],\n",
       "          [253, 253, 253,  ..., 176, 185, 195]]], dtype=torch.uint8),\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d5c86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True) #no_workers, pin_memory, drop_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "052e8519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([4, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([4, 1])\n",
      "Batch of images has shape:  torch.Size([1, 3, 480, 480])\n",
      "Batch of labels has shape:  torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in data_loader:\n",
    "    print(\"Batch of images has shape: \",imgs.shape)\n",
    "    print(\"Batch of labels has shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a3d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbf200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a6fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a34e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dades \n",
    "trans = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])   \n",
    "\n",
    "train_data = torchvision.datasets.MNIST('/files/', train=True,  download=True, transform=trans)\n",
    "test_data  = torchvision.datasets.MNIST('/files/', train=False, download=True, transform=trans) \n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(train_data,batch_size=batch_size, shuffle=True)\n",
    "test_loader =  data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# exemple de mostra\n",
    "idx = 3                       #un qualsevol per veure com són\n",
    "input_example,target_example = train_data.__getitem__(idx)\n",
    "print(input_example.size())       #mida de cada mostra/imatge/digit\n",
    "plt.imshow(input_example[0,:,:], cmap='gray')\n",
    "plt.title(str(target_example))\n",
    "plt.show()\n",
    "\n",
    "# exemple de batch\n",
    "dataiter = iter(train_loader)\n",
    "batch_images_example, batch_labels_example = dataiter.next()\n",
    "\n",
    "plt.imshow(torchvision.utils.make_grid(batch_images_example)[0,:,:], cmap='gray')\n",
    "plt.title('batch example')\n",
    "plt.show()\n",
    "print(batch_labels_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc691a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52ce1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
